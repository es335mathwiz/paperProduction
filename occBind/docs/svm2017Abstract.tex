\documentclass[12pt]{article}
\title{Using Support Vector Machine Function Approximation to
Mitigate the Curse of Dimensionality in Models with Occasionally Binding Constraints}
\author{Gary S. Anderson}
\begin{document}
\begin{abstract}


This paper applies support vector machine function approximation to
reduce the computational burden associated with representing the
unknown, potentially highly nonlinear stochstic functions that arise in
solving dynamic models with occasionally binding constraints.


Support vector machines 
have become an essential tool in contemporary machine learning research
where computer scientists exploit their flexibility and
computational tractability in modelling complex high dimensional data.
Like many other function approximation approaches,
support vector machines represent functions as a linearly weighted sum
of a family of basis functions.  They differ from other approaches in  the
use of ``hinge loss functions'' that generate
an easy to solve
quadratic programming problem(QPP) for determining the weights.
The solution of this QPP identifies a subset of points, the support vectors,
that are influential in the representation.  With strategically chosen
basis functions, this can dramatically reduce the number of terms needed
to approximate a function to a given level of accuracy.


This paper uses a series representation for bounded solutions
to dynamic models to compute
  time invariant discrete time
maps that accurately characterize
the solutions for a wide array of nonlinear
rational expectations models. This solution also provides a formula for computing accuracy bounds for any proposed time invariant model solution.
Support vector machine function approximation provides
error bounds that 
are especially useful used in conjuction with the series representation
error formulae.
One can  dynamically adjust the representation accuracy
as needed to guarantee convergence to a true solution.

\end{abstract}
\end{document}
 